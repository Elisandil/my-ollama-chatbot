# ğŸ¤– Chatbot con Llama3 via Ollama

Chatbot inteligente construido con LangChain, Ollama y Gradio, con gestiÃ³n avanzada de sesiones y persistencia de conversaciones.

## âœ¨ CaracterÃ­sticas

- ğŸ”„ **Streaming de respuestas** en tiempo real
- ğŸ’¾ **Persistencia de historial** con SQLite
- ğŸ” **GestiÃ³n de sesiones** por usuario con UUID Ãºnico
- â±ï¸ **Heartbeat automÃ¡tico** para mantener sesiones activas
- ğŸ§¹ **Limpieza automÃ¡tica** de sesiones inactivas
- ğŸ“ **Logging completo** para debugging
- âš™ï¸ **ConfiguraciÃ³n flexible** mediante variables de entorno
- ğŸ›¡ï¸ **Manejo robusto de errores**

## ğŸ“‹ Requisitos Previos

- Python 3.8+
- Ollama instalado y en ejecuciÃ³n
- Modelo Llama3 descargado en Ollama

## ğŸš€ InstalaciÃ³n

### 1. Instalar Ollama

```bash
# macOS/Linux
curl -fsSL https://ollama.com/install.sh | sh

# Windows: descargar desde https://ollama.com/download
```

### 2. Descargar el modelo Llama3

```bash
ollama pull llama3:latest
```

### 3. Clonar e instalar dependencias

```bash
# Clonar el repositorio
git clone <tu-repositorio>
cd chatbot-llama3

# Crear entorno virtual
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt
```

### 4. Configurar variables de entorno (opcional)

```bash
# Copiar archivo de ejemplo
cp .env.example .env

# Editar segÃºn tus necesidades
nano .env
```

## ğŸ® Uso

### Iniciar el chatbot

```bash
python run.py
```

El navegador se abrirÃ¡ automÃ¡ticamente en `http://localhost:7860`

### Verificar que Ollama estÃ¡ activo

```bash
# En otra terminal
ollama list
```

## ğŸ—ï¸ Estructura del Proyecto

```
app/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ config.py              # ConfiguraciÃ³n centralizada
â”œâ”€â”€ gradio_app.py          # AplicaciÃ³n principal
â”œâ”€â”€ ui.py                  # Interfaz de Gradio
â”œâ”€â”€ chatbot_logic.py       # LÃ³gica del chatbot
â”œâ”€â”€ session_manager.py     # GestiÃ³n de sesiones
â””â”€â”€ history_manager.py     # GestiÃ³n de historial

run.py                     # Punto de entrada
requirements.txt           # Dependencias
.env.example              # Ejemplo de configuraciÃ³n
```

## âš™ï¸ ConfiguraciÃ³n

### Variables de Entorno Disponibles

| Variable | DescripciÃ³n | Por Defecto |
|----------|-------------|-------------|
| `MODEL_NAME` | Modelo de Ollama a usar | `llama3:latest` |
| `OLLAMA_BASE_URL` | URL del servidor Ollama | `http://localhost:11434` |
| `MAX_TOKENS` | MÃ¡ximo de tokens por respuesta | `2048` |
| `SESSION_TIMEOUT` | Timeout de sesiÃ³n (segundos) | `600` (10 min) |
| `DB_PATH` | Ruta de base de datos | `sqlite:///chat_history.db` |
| `LOG_LEVEL` | Nivel de logging | `INFO` |
| `GRADIO_SERVER_PORT` | Puerto del servidor | `7860` |
| `MAX_MESSAGE_LENGTH` | Longitud mÃ¡xima de mensaje | `4000` |
| `MAX_HISTORY_MESSAGES` | Mensajes mÃ¡ximos en historial | `50` |

## ğŸ”§ Mejoras Implementadas

### Respecto a la VersiÃ³n Original

1. âœ… **Sistema de sesiones corregido**: Usa UUID Ãºnico por usuario en lugar de PID compartido
2. âœ… **Prompt mejorado**: Sistema prompt mÃ¡s Ãºtil y especÃ­fico
3. âœ… **Manejo de errores robusto**: Try-catch en todas las operaciones crÃ­ticas
4. âœ… **ValidaciÃ³n de Ollama**: Verifica disponibilidad antes de iniciar
5. âœ… **ConfiguraciÃ³n externa**: Variables de entorno en lugar de hardcoded
6. âœ… **Logging apropiado**: Sistema de logging completo
7. âœ… **Thread-safe**: Uso de locks para operaciones concurrentes
8. âœ… **ValidaciÃ³n de entrada**: SanitizaciÃ³n de inputs del usuario
9. âœ… **UI mejorada**: Ejemplos, mejor UX, temas de Gradio

## ğŸ› SoluciÃ³n de Problemas

### Error: "No se puede conectar con Ollama"

```bash
# Verificar que Ollama estÃ¡ activo
ollama serve

# En otra terminal
ollama list
```

### Error: "Modelo no encontrado"

```bash
# Descargar el modelo
ollama pull llama3:latest
```

### Sesiones expiran demasiado rÃ¡pido

```bash
# Aumentar el timeout en .env
SESSION_TIMEOUT=1800  # 30 minutos
```

### Base de datos bloqueada

```bash
# Eliminar la base de datos y reiniciar
rm chat_history.db
python run.py
```

## ğŸ“Š Monitoreo

El sistema genera logs detallados:

```
2024-XX-XX XX:XX:XX - session_manager - INFO - SesiÃ³n registrada: 12345678...
2024-XX-XX XX:XX:XX - chatbot_logic - INFO - Procesando pregunta para sesiÃ³n 12345678...
2024-XX-XX XX:XX:XX - session_manager - INFO - Limpiadas 2 sesiones. Activas: 5
```

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas. Por favor:

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## ğŸ“ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT.

## ğŸ‘¤ Autor

**Antonio Ortega**

## ğŸ™ Agradecimientos

- [LangChain](https://python.langchain.com/) - Framework de LLM
- [Ollama](https://ollama.com/) - Servidor de modelos locales
- [Gradio](https://gradio.app/) - UI para ML
- [Meta](https://ai.meta.com/) - Modelo Llama3